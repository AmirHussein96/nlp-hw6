{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file illustrates how you might experiment with the HMM interface at the prompt.\n",
    "# You can also run it directly.\n",
    "\n",
    "import logging, math, os\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "from corpus import TaggedCorpus, sentence_str\n",
    "from eval import model_cross_entropy, tagger_write_output\n",
    "from hmm_test import HiddenMarkovModel\n",
    "from lexicon import build_lexicon\n",
    "import torch\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 40 tokens from icsup\n",
      "INFO : Created 4 tag types\n",
      "INFO : Created 5 word types\n",
      "INFO : Ice cream vocabulary: ['1', '2', '3', '_EOS_WORD_', '_BOS_WORD_']\n",
      "INFO : Ice cream tagset: ['C', 'H', '_EOS_TAG_', '_BOS_TAG_']\n",
      "INFO : *** Current A, B matrices (computed by softmax from small random parameters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A:\n",
      "\tC\tH\t_EOS_TAG_\t_BOS_TAG_\n",
      "C\t0.334\t0.332\t0.334\t0.000\n",
      "H\t0.332\t0.334\t0.334\t0.000\n",
      "_EOS_TAG_\t0.333\t0.335\t0.332\t0.000\n",
      "_BOS_TAG_\t0.333\t0.333\t0.334\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\n",
      "C\t0.332\t0.334\t0.334\n",
      "H\t0.332\t0.334\t0.334\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(format=\"%(levelname)s : %(message)s\", level=logging.INFO)  # could change INFO to DEBUG\n",
    "# torch.autograd.set_detect_anomaly(True)    # uncomment to improve error messages from .backward(), but slows down\n",
    "\n",
    "# Make an HMM with randomly initialized parameters.\n",
    "icsup = TaggedCorpus(Path(\"../nlp6-data/icsup\"), add_oov=False)\n",
    "logging.info(f\"Ice cream vocabulary: {list(icsup.vocab)}\")\n",
    "logging.info(f\"Ice cream tagset: {list(icsup.tagset)}\")\n",
    "lexicon = build_lexicon(icsup, one_hot=True)   # one-hot lexicon: separate parameters for each word\n",
    "hmm = HiddenMarkovModel(icsup.tagset, icsup.vocab, lexicon)\n",
    "\n",
    "logging.info(\"*** Current A, B matrices (computed by softmax from small random parameters)\")\n",
    "hmm.updateAB()   # compute the matrices from the initial parameters (this would normally happen during training).\n",
    "                 # An alternative is to set them directly to some spreadsheet values you'd like to try.\n",
    "# hmm.A = torch.Tensor([[0.8, 0.1,0.1,0],[0.1,0.8,0.1,0],[0,0,0,0],[0.5, 0.5,0,0]])\n",
    "# hmm.B = torch.Tensor([[0.7, 0.2,0.1],[0.1,0.2,0.7],[0,0,0],[0,0,0]])\n",
    "hmm.printAB()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While training on ice cream, we will just evaluate the cross-entropy\n",
    "# on the training data itself (icsup), since we are interested in watching it improve.\n",
    "logging.info(\"*** Supervised training on icsup\")\n",
    "cross_entropy_loss = lambda model: model_cross_entropy(model, icsup)\n",
    "hmm.train(corpus=icsup, loss=cross_entropy_loss, \n",
    "          minibatch_size=10, evalbatch_size=500, lr=0.01, tolerance=0.0001)\n",
    "\n",
    "logging.info(\"*** A, B matrices after training on icsup (should approximately match initial params on spreadsheet [transposed])\")\n",
    "hmm.printAB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Loading model from my_hmm.pkl\n",
      "INFO : Loaded model from my_hmm.pkl\n",
      "INFO : *** Viterbi results on icraw\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: tensor([0, 0, 0, 0]), 2: tensor([0, 0, 0, 0]), 3: tensor([0, 0, 0, 0]), 4: tensor([0, 0, 0, 0]), 5: tensor([0, 0, 0, 0]), 6: tensor([0, 0, 0, 0]), 7: tensor([0, 0, 0, 0]), 8: tensor([0, 0, 0, 0]), 9: tensor([0, 0, 0, 0]), 10: tensor([0, 0, 0, 0]), 11: tensor([0, 0, 0, 0]), 12: tensor([0, 0, 0, 0]), 13: tensor([0, 0, 0, 0]), 14: tensor([0, 0, 0, 0]), 15: tensor([0, 0, 0, 0]), 16: tensor([0, 0, 0, 0]), 17: tensor([0, 0, 0, 0]), 18: tensor([0, 0, 0, 0]), 19: tensor([0, 0, 0, 0]), 20: tensor([0, 0, 0, 0]), 21: tensor([0, 0, 0, 0]), 22: tensor([0, 0, 0, 0]), 23: tensor([0, 0, 0, 0]), 24: tensor([0, 0, 0, 0]), 25: tensor([0, 0, 0, 0]), 26: tensor([0, 0, 0, 0]), 27: tensor([0, 0, 0, 0]), 28: tensor([0, 0, 0, 0]), 29: tensor([0, 0, 0, 0]), 30: tensor([0, 0, 0, 0]), 31: tensor([0, 0, 0, 0]), 32: tensor([0, 0, 0, 0]), 33: tensor([0, 0, 0, 0]), 34: tensor([0, 0, 0, 0])}\n",
      "_EOS_TAG_\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n",
      "tensor(0)\n",
      "C\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-90260e44ed7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*** Viterbi results on icraw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0micraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTaggedCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../nlp6-data/icraw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0micsup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0micsup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtagger_write_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0micraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"icraw.output\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# calls hmm.viterbi_tagging on each sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cat ../nlp6-data/icraw.output\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# print the file we just created, and remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlp_hw/nlp-hw6/eval.py\u001b[0m in \u001b[0;36mtagger_write_output\u001b[0;34m(model, eval_corpus, output_path)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviterbi_tagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesupervise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlp_hw/nlp-hw6/hmm_test.py\u001b[0m in \u001b[0;36mviterbi_tagging\u001b[0;34m(self, sentence, corpus)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mprev_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackpointer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "hmm.load('my_hmm.pkl')\n",
    "logging.info(\"*** Viterbi results on icraw\")\n",
    "icraw = TaggedCorpus(Path(\"../nlp6-data/icraw\"), tagset=icsup.tagset, vocab=icsup.vocab)\n",
    "tagger_write_output(hmm, icraw, Path(\"icraw.output\"))  # calls hmm.viterbi_tagging on each sentence\n",
    "os.system(\"cat ../nlp6-data/icraw.output\")   # print the file we just created, and remove it\n",
    "\n",
    "# Now let's use the forward algorithm to see what the model thinks about \n",
    "# the probability of the spreadsheet \"sentence.\"\n",
    "logging.info(\"*** Forward algorithm on icraw (should approximately match iteration 0 \"\n",
    "             \"on spreadsheet)\")\n",
    "for sentence in icraw:\n",
    "    prob = math.exp(hmm.log_prob(sentence, icraw))\n",
    "    logging.info(f\"{prob} = p({sentence_str(sentence)})\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's reestimate on the icraw data, as the spreadsheet does.\n",
    "logging.info(\"*** Reestimating on icraw (perplexity should improve on every iteration)\")\n",
    "negative_log_likelihood = lambda model: model_cross_entropy(model, icraw)  # evaluate on icraw itself\n",
    "hmm.train(corpus=icraw, loss=negative_log_likelihood,\n",
    "          minibatch_size=10, evalbatch_size=500, lr=0.001, tolerance=0.0001)\n",
    "\n",
    "logging.info(\"*** A, B matrices after reestimation on icraw (SGD, not EM, but still \"\n",
    "             \"should approximately match final params on spreadsheet [transposed])\")\n",
    "hmm.printAB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ddf02f4c0292df6831d782027890daba6803fd8ec186dc3e6f7222b70d5e314"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
