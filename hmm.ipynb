{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file illustrates how you might experiment with the HMM interface at the prompt.\n",
    "# You can also run it directly.\n",
    "\n",
    "import logging, math, os\n",
    "from pathlib import Path\n",
    "from corpus import TaggedCorpus, desupervise, sentence_str\n",
    "from typing import Callable\n",
    "from corpus import TaggedCorpus, sentence_str\n",
    "\n",
    "\n",
    "from eval import model_cross_entropy, tagger_write_output\n",
    "from hmm import HiddenMarkovModel\n",
    "from crf import CRFModel\n",
    "from lexicon import build_lexicon\n",
    "import torch\n",
    "\n",
    "import pdb\n",
    "from eval import eval_tagging, model_cross_entropy, model_error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 40 tokens from icsup\n",
      "INFO : Created 4 tag types\n",
      "INFO : Created 5 word types\n",
      "INFO : Ice cream vocabulary: ['1', '2', '3', '_EOS_WORD_', '_BOS_WORD_']\n",
      "INFO : Ice cream tagset: ['C', 'H', '_EOS_TAG_', '_BOS_TAG_']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'lexicon'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-60104ddf64bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Ice cream tagset: {list(icsup.tagset)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0micsup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# one-hot lexicon: separate parameters for each word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mhmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHiddenMarkovModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0micsup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0micsup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mhmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_hmm.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*** Current A, B matrices (computed by softmax from small random parameters)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'lexicon'"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(format=\"%(levelname)s : %(message)s\", level=logging.INFO)  # could change INFO to DEBUG\n",
    "# torch.autograd.set_detect_anomaly(True)    # uncomment to improve error messages from .backward(), but slows down\n",
    "\n",
    "# Make an HMM with randomly initialized parameters.\n",
    "icsup = TaggedCorpus(Path(\"../nlp6-data/icsup\"), add_oov=False)\n",
    "logging.info(f\"Ice cream vocabulary: {list(icsup.vocab)}\")\n",
    "logging.info(f\"Ice cream tagset: {list(icsup.tagset)}\")\n",
    "lexicon = build_lexicon(icsup, one_hot=True)   # one-hot lexicon: separate parameters for each word\n",
    "hmm = HiddenMarkovModel(icsup.tagset, icsup.vocab, lexicon)\n",
    "hmm = hmm.load('my_hmm.pkl')\n",
    "logging.info(\"*** Current A, B matrices (computed by softmax from small random parameters)\")\n",
    "#hmm.updateAB()   # compute the matrices from the initial parameters (this would normally happen during training).\n",
    "                 # An alternative is to set them directly to some spreadsheet values you'd like to try.\n",
    "# hmm.A = torch.Tensor([[0.8, 0.1,0.1,0],[0.1,0.8,0.1,0],[0,0,0,0],[0.5, 0.5,0,0]])\n",
    "# hmm.B = torch.Tensor([[0.7, 0.2,0.1],[0.1,0.2,0.7],[0,0,0],[0,0,0]])\n",
    "hmm.printAB()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While training on ice cream, we will just evaluate the cross-entropy\n",
    "# on the training data itself (icsup), since we are interested in watching it improve.\n",
    "logging.info(\"*** Supervised training on icsup\")\n",
    "cross_entropy_loss = lambda model: model_cross_entropy(model, icsup)\n",
    "hmm.train(corpus=icsup, loss=cross_entropy_loss, \n",
    "          minibatch_size=10, evalbatch_size=500, lr=0.01, tolerance=0.0001)\n",
    "\n",
    "logging.info(\"*** A, B matrices after training on icsup (should approximately match initial params on spreadsheet [transposed])\")\n",
    "hmm.printAB()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Loading model from my_hmm.pkl\n",
      "INFO : Loaded model from my_hmm.pkl\n",
      "INFO : *** Viterbi results on icraw\n",
      "1it [00:00, 31.56it/s]\n",
      "INFO : *** Forward algorithm on icraw (should approximately match iteration 0 on spreadsheet)\n",
      "INFO : 1.4301312227198852e-58 = p(2 3 3 2 3 2 3 2 2 3 1 3 3 1 1 1 2 1 1 1 3 1 2 1 1 1 2 3 3 2 3 2 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: tensor([-inf, -inf, -inf, 0.]), 1: tensor([3, 3, 3, 3]), 2: tensor([1, 1, 1, 1]), 3: tensor([1, 1, 1, 1]), 4: tensor([1, 1, 1, 1]), 5: tensor([1, 1, 1, 1]), 6: tensor([1, 1, 1, 1]), 7: tensor([1, 1, 1, 1]), 8: tensor([1, 1, 1, 1]), 9: tensor([1, 1, 1, 1]), 10: tensor([1, 1, 1, 1]), 11: tensor([0, 1, 0, 0]), 12: tensor([0, 1, 0, 0]), 13: tensor([0, 1, 1, 1]), 14: tensor([0, 0, 0, 0]), 15: tensor([0, 0, 0, 0]), 16: tensor([0, 0, 0, 0]), 17: tensor([0, 1, 0, 0]), 18: tensor([0, 0, 0, 0]), 19: tensor([0, 0, 0, 0]), 20: tensor([0, 0, 0, 0]), 21: tensor([0, 1, 0, 0]), 22: tensor([0, 0, 0, 0]), 23: tensor([0, 1, 0, 0]), 24: tensor([0, 0, 0, 0]), 25: tensor([0, 0, 0, 0]), 26: tensor([0, 0, 0, 0]), 27: tensor([0, 1, 0, 0]), 28: tensor([0, 1, 0, 0]), 29: tensor([0, 1, 1, 1]), 30: tensor([0, 1, 1, 1]), 31: tensor([1, 1, 1, 1]), 32: tensor([1, 1, 1, 1]), 33: tensor([1, 1, 1, 1]), 34: tensor([0, 1, 1, 1])}\n",
      "_EOS_TAG_\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "3\n",
      "_BOS_TAG_\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "hmm = hmm.load('my_hmm.pkl')\n",
    "logging.info(\"*** Viterbi results on icraw\")\n",
    "icraw = TaggedCorpus(Path(\"../nlp6-data/icraw\"), tagset=icsup.tagset, vocab=icsup.vocab)\n",
    "tagger_write_output(hmm, icraw, Path(\"icraw.output\"))  # calls hmm.viterbi_tagging on each sentence\n",
    "os.system(\"cat ../nlp6-data/icraw.output\")   # print the file we just created, and remove it\n",
    "\n",
    "# Now let's use the forward algorithm to see what the model thinks about \n",
    "# the probability of the spreadsheet \"sentence.\"\n",
    "logging.info(\"*** Forward algorithm on icraw (should approximately match iteration 0 \"\n",
    "             \"on spreadsheet)\")\n",
    "for sentence in icraw:\n",
    "    prob = math.exp(hmm.log_prob(sentence, icraw))\n",
    "    logging.info(f\"{prob} = p({sentence_str(sentence)})\")\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : *** Reestimating on icraw (perplexity should improve on every iteration)\n",
      "1it [00:00, 77.69it/s]\n",
      "INFO : Cross-entropy: 3.9497 nats (= perplexity 51.921)\n",
      "1it [00:00, 139.47it/s]\n",
      "INFO : Cross-entropy: 3.9494 nats (= perplexity 51.905)\n",
      "INFO : Saved model to my_hmm.pkl\n",
      "500it [00:14, 35.62it/s]\n",
      "INFO : *** A, B matrices after reestimation on icraw (SGD, not EM, but still should approximately match final params on spreadsheet [transposed])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A:\n",
      "\tC\tH\t_EOS_TAG_\t_BOS_TAG_\n",
      "C\t0.930\t0.069\t0.001\t0.000\n",
      "H\t0.084\t0.915\t0.001\t0.000\n",
      "_EOS_TAG_\t0.333\t0.334\t0.333\t0.000\n",
      "_BOS_TAG_\t0.072\t0.924\t0.003\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\n",
      "C\t0.650\t0.153\t0.197\n",
      "H\t0.014\t0.484\t0.502\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finally, let's reestimate on the icraw data, as the spreadsheet does.\n",
    "logging.info(\"*** Reestimating on icraw (perplexity should improve on every iteration)\")\n",
    "negative_log_likelihood = lambda model: model_cross_entropy(model, icraw)  # evaluate on icraw itself\n",
    "hmm.train(corpus=icraw, loss=negative_log_likelihood,\n",
    "          minibatch_size=10, evalbatch_size=500, lr=0.001, tolerance=0.0001)\n",
    "\n",
    "logging.info(\"*** A, B matrices after reestimation on icraw (SGD, not EM, but still \"\n",
    "             \"should approximately match final params on spreadsheet [transposed])\")\n",
    "hmm.printAB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading model from ic_crf.pkl\n",
      "INFO:root:Loaded model from ic_crf.pkl\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The corpus that this sentence came from uses a different tagset or vocab",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22352/2896453956.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mic_crf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCRFModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ic_crf.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_error_rate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mic_crf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_corpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0micsup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\lenovo\\Desktop\\2021 Fall\\Natural Language Processing\\hw\\tagging\\eval.py\u001b[0m in \u001b[0;36mmodel_error_rate\u001b[1;34m(model, eval_corpus, known_vocab)\u001b[0m\n\u001b[0;32m     42\u001b[0m     after printing cross-entropy and a breakdown of accuracy (using the logger).\"\"\"\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mmodel_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_corpus\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# call for side effects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     return tagger_error_rate(viterbi_tagger(model, eval_corpus),\n\u001b[0;32m     46\u001b[0m                              \u001b[0meval_corpus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lenovo\\Desktop\\2021 Fall\\Natural Language Processing\\hw\\tagging\\eval.py\u001b[0m in \u001b[0;36mmodel_cross_entropy\u001b[1;34m(model, eval_corpus)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mtoken_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_corpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mlog_prob\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mtoken_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m    \u001b[1;31m# count EOS but not BOS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mcross_entropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlog_prob\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtoken_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lenovo\\Desktop\\2021 Fall\\Natural Language Processing\\hw\\tagging\\crf.py\u001b[0m in \u001b[0;36mlog_prob\u001b[1;34m(self, sentence, corpus)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesupervise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mlog_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTaggedCorpus\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         \"\"\"Run the forward algorithm from the handout on a tagged, untagged, \n\u001b[0;32m    185\u001b[0m         or partially tagged sentence.  Return log Z (the log of the forward \n",
      "\u001b[1;32mc:\\Users\\lenovo\\Desktop\\2021 Fall\\Natural Language Processing\\hw\\tagging\\crf.py\u001b[0m in \u001b[0;36mlog_forward\u001b[1;34m(self, sentence, corpus)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_integerize_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[1;31m# The \"nice\" way to construct alpha is by appending to a List[Tensor] at each\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m         \u001b[1;31m# step.  But to better match the notation in the handout, we'll instead preallocate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;31m# a list of length n+2 so that we can assign directly to alpha[j].\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lenovo\\Desktop\\2021 Fall\\Natural Language Processing\\hw\\tagging\\crf.py\u001b[0m in \u001b[0;36m_integerize_sentence\u001b[1;34m(self, sentence, corpus)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The corpus that this sentence came from uses a different tagset or vocab\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[1;31m# If so, go ahead and integerize it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintegerize_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: The corpus that this sentence came from uses a different tagset or vocab"
     ]
    }
   ],
   "source": [
    "ic_crf = CRFModel.load('ic_crf.pkl')\n",
    "model_error_rate(ic_crf, eval_corpus=icsup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hmm on En data\n",
    "# Get the corpora\n",
    "entrain = TaggedCorpus(Path(\"../nlp6-data/ensup\"), Path(\"../nlp6-data/enraw\"))                               # all training\n",
    "ensup =   TaggedCorpus(Path(\"../nlp6-data/ensup\"), tagset=entrain.tagset, vocab=entrain.vocab)  # supervised training\n",
    "endev =   TaggedCorpus(Path(\"../nlp6-data/endev\"), tagset=entrain.tagset, vocab=entrain.vocab)  # evaluation\n",
    "logging.info(f\"Tagset: f{list(entrain.tagset)}\")\n",
    "known_vocab = TaggedCorpus(Path(\"../nlp6-data/ensup\")).vocab    # words seen with supervised tags; used in evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an HMM\n",
    "#lexicon = build_lexicon(entrain, embeddings_file=Path('../lexicons/words-50.txt'))  # works better with more attributes!\n",
    "hmm = HiddenMarkovModel.load('en_hmm.pkl')\n",
    "\n",
    "#hmm = hmm.load('en_hmm.pkl')\n",
    "#hmm = hmm.load('divsup_train.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "996it [00:05, 181.28it/s]\n",
      "INFO:root:Cross-entropy: 8.0400 nats (= perplexity 3102.636)\n",
      "996it [00:06, 148.22it/s]\n",
      "INFO:root:Tagging accuracy: all: 81.739%, known: 83.175%, seen: 72.391%, novel: 62.814%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18260668439921357"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "model_error_rate(hmm, eval_corpus=endev, known_vocab=known_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading model from en_hmm_awesome.pkl\n",
      "INFO:root:Loaded model from en_hmm_awesome.pkl\n",
      "996it [00:05, 179.65it/s]\n",
      "INFO:root:Cross-entropy: 6.9604 nats (= perplexity 1054.084)\n",
      "996it [00:04, 242.16it/s]\n",
      "INFO:root:Tagging accuracy: all: 93.524%, known: 96.094%, seen: 67.340%, novel: 63.342%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06476234532207703"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_aw = HiddenMarkovModel.load('en_hmm_awesome.pkl')\n",
    "model_error_rate(hmm_aw, eval_corpus=endev, known_vocab=known_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awesome tag\n",
    "\n",
    "# load hmm on En data\n",
    "# Get the corpora\n",
    "entrain = TaggedCorpus(Path(\"../nlp6-data/ensup\"), Path(\"../nlp6-data/enraw\"), , log_counts=True)                # all training\n",
    "ensup =   TaggedCorpus(Path(\"../nlp6-data/ensup\"), tagset=entrain.tagset, vocab=entrain.vocab, log_counts=True)  # supervised training\n",
    "endev =   TaggedCorpus(Path(\"../nlp6-data/endev\"), tagset=entrain.tagset, vocab=entrain.vocab, log_counts=True)  # evaluation\n",
    "logging.info(f\"Tagset: f{list(entrain.tagset)}\")\n",
    "known_vocab = TaggedCorpus(Path(\"../nlp6-data/ensup\")).vocab    # words seen with supervised tags; used in evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "996it [00:04, 199.85it/s]\n",
      "996it [00:03, 278.55it/s]\n",
      "996it [00:05, 195.63it/s]\n",
      "996it [00:03, 279.85it/s]\n",
      "10000it [05:17, 31.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# train on unsupervised\n",
    "logger=logging.getLogger() \n",
    "\n",
    "#Now we are going to Set the threshold of logger to DEBUG \n",
    "logger.setLevel(logging.INFO) \n",
    "\n",
    "loss_dev = lambda model: model_error_rate(model, eval_corpus=endev, known_vocab=known_vocab)\n",
    "hmm.train(corpus=entrain, loss=loss_dev, minibatch_size=30, evalbatch_size=10000, lr=0.0001, reg=0,save_path='unsup_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Gold:    ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/N added/V ,/, ``/` and/C that/D means/V virtually/R everyone/N who/W works/V here/R ./.\n",
      "INFO:root:Viterbi: ``/P We/V 're/R strongly/- _OOV_/W that/U anyone/W who/V has/V eaten/I in/D the/N cafeteria/D this/N month/V have/D the/N shot/, ,/' ''/' Mr./C Mattausch/N added/, ,/C ``/C and/W that/V means/R virtually/U everyone/W who/S works/R here/. ./'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1388888888888889\n",
      "0.8611111111111112\n"
     ]
    }
   ],
   "source": [
    "logger=logging.getLogger() \n",
    "\n",
    "#Now we are going to Set the threshold of logger to DEBUG \n",
    "logger.setLevel(logging.DEBUG) \n",
    "acc = 0\n",
    "c =0\n",
    "for m, sentence in enumerate(endev):\n",
    "    if m >= 1: break\n",
    "    viterbi = hmm.viterbi_tagging(desupervise(sentence), endev)\n",
    "    counts = eval_tagging(predicted=viterbi, gold=sentence, \n",
    "                          known_vocab=known_vocab)\n",
    "    num = counts['NUM', 'ALL']\n",
    "    denom = counts['DENOM', 'ALL']\n",
    "  \n",
    "    logging.info(f\"Gold:    {sentence_str(sentence)}\")\n",
    "    logging.info(f\"Viterbi: {sentence_str(viterbi)}\")\n",
    "    acc_all = (denom - num)/denom\n",
    "    print(num/denom)\n",
    "   # logging.info(f\"acc:    {num}/{denom}\")\n",
    "    acc += acc_all \n",
    "    \n",
    "    c+=1\n",
    "print(acc/c)\n",
    "    # logging.info(f\"Prob:    {math.exp(hmm.log_prob(sentence, endev))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('DENOM', 'KNOWN'): 33,\n",
       "         ('NUM', 'KNOWN'): 4,\n",
       "         ('DENOM', 'ALL'): 36,\n",
       "         ('NUM', 'ALL'): 5,\n",
       "         ('DENOM', 'NOVEL'): 1,\n",
       "         ('DENOM', 'SEEN'): 2,\n",
       "         ('NUM', 'SEEN'): 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Read 73 tokens from icsup, icraw\n",
      "INFO:root:Created 4 tag types\n",
      "INFO:root:Created 6 word types\n",
      "INFO:root:Tagset: f['C', 'H', '_EOS_TAG_', '_BOS_TAG_']\n",
      "INFO:root:Read 40 tokens from icsup\n",
      "INFO:root:Created 4 tag types\n",
      "INFO:root:Created 6 word types\n"
     ]
    }
   ],
   "source": [
    "# load hmm on ic data\n",
    "# Get the corpora\n",
    "ictrain = TaggedCorpus(Path(\"../nlp6-data/icsup\"), Path(\"../nlp6-data/icraw\"))                               # all training\n",
    "icsup =   TaggedCorpus(Path(\"../nlp6-data/icsup\"), tagset=ictrain.tagset, vocab=ictrain.vocab)  # supervised training\n",
    "icdev =   TaggedCorpus(Path(\"../nlp6-data/icdev\"), tagset=ictrain.tagset, vocab=ictrain.vocab)  # evaluation\n",
    "logging.info(f\"Tagset: f{list(ictrain.tagset)}\")\n",
    "\n",
    "known_vocab = TaggedCorpus(Path(\"../nlp6-data/icsup\")).vocab    # words seen with supervised tags; used in evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:*** Current A, B matrices (computed by softmax from small random parameters)\n",
      "INFO:root:*** Supervised training on icsup\n",
      "4it [00:00, 232.36it/s]\n",
      "INFO:root:Cross-entropy: 2.2305 nats (= perplexity 9.304)\n",
      "4it [00:00, 518.70it/s]\n",
      "INFO:root:Cross-entropy: 1.1334 nats (= perplexity 3.106)\n",
      "4it [00:00, 525.57it/s]\n",
      "INFO:root:Cross-entropy: 1.1079 nats (= perplexity 3.028)\n",
      "4it [00:00, 530.35it/s]\n",
      "INFO:root:Cross-entropy: 1.0982 nats (= perplexity 2.999)\n",
      "4it [00:00, 511.55it/s]\n",
      "INFO:root:Cross-entropy: 1.0931 nats (= perplexity 2.984)\n",
      "4it [00:00, 502.97it/s]\n",
      "INFO:root:Cross-entropy: 1.0900 nats (= perplexity 2.974)\n",
      "4it [00:00, 500.38it/s]\n",
      "INFO:root:Cross-entropy: 1.0879 nats (= perplexity 2.968)\n",
      "4it [00:00, 520.21it/s]\n",
      "INFO:root:Cross-entropy: 1.0864 nats (= perplexity 2.964)\n",
      "4it [00:00, 509.00it/s]\n",
      "INFO:root:Cross-entropy: 1.0853 nats (= perplexity 2.960)\n",
      "4it [00:00, 508.23it/s]\n",
      "INFO:root:Cross-entropy: 1.0844 nats (= perplexity 2.958)\n",
      "4it [00:00, 520.72it/s]\n",
      "INFO:root:Cross-entropy: 1.0837 nats (= perplexity 2.956)\n",
      "4it [00:00, 526.05it/s]\n",
      "INFO:root:Cross-entropy: 1.0832 nats (= perplexity 2.954)\n",
      "4it [00:00, 528.77it/s]\n",
      "INFO:root:Cross-entropy: 1.0827 nats (= perplexity 2.953)\n",
      "4it [00:00, 535.62it/s]\n",
      "INFO:root:Cross-entropy: 1.0823 nats (= perplexity 2.951)\n",
      "4it [00:00, 530.94it/s]\n",
      "INFO:root:Cross-entropy: 1.0819 nats (= perplexity 2.950)\n",
      "4it [00:00, 535.24it/s]\n",
      "INFO:root:Cross-entropy: 1.0816 nats (= perplexity 2.949)\n",
      "4it [00:00, 482.88it/s]\n",
      "INFO:root:Cross-entropy: 1.0813 nats (= perplexity 2.949)\n",
      "4it [00:00, 514.32it/s]\n",
      "INFO:root:Cross-entropy: 1.0811 nats (= perplexity 2.948)\n",
      "4it [00:00, 517.22it/s]\n",
      "INFO:root:Cross-entropy: 1.0809 nats (= perplexity 2.947)\n",
      "4it [00:00, 389.42it/s]\n",
      "INFO:root:Cross-entropy: 1.0807 nats (= perplexity 2.947)\n",
      "4it [00:00, 399.72it/s]\n",
      "INFO:root:Cross-entropy: 1.0805 nats (= perplexity 2.946)\n",
      "4it [00:00, 414.41it/s]\n",
      "INFO:root:Cross-entropy: 1.0804 nats (= perplexity 2.946)\n",
      "4it [00:00, 526.69it/s]\n",
      "INFO:root:Cross-entropy: 1.0803 nats (= perplexity 2.945)\n",
      "4it [00:00, 536.30it/s]\n",
      "INFO:root:Cross-entropy: 1.0801 nats (= perplexity 2.945)\n",
      "4it [00:00, 520.09it/s]\n",
      "INFO:root:Cross-entropy: 1.0800 nats (= perplexity 2.945)\n",
      "4it [00:00, 485.21it/s]\n",
      "INFO:root:Cross-entropy: 1.0799 nats (= perplexity 2.944)\n",
      "INFO:root:Saved model to icsup_train.pkl\n",
      "12500it [01:34, 131.91it/s]\n"
     ]
    }
   ],
   "source": [
    "logger=logging.getLogger() \n",
    "\n",
    "#Now we are going to Set the threshold of logger to DEBUG \n",
    "logger.setLevel(logging.INFO) \n",
    "lexicon = build_lexicon(ictrain, one_hot=True)   # one-hot lexicon: separate parameters for each word\n",
    "hmm = HiddenMarkovModel(ictrain.tagset, ictrain.vocab, lexicon)\n",
    "\n",
    "logging.info(\"*** Current A, B matrices (computed by softmax from small random parameters)\")\n",
    "hmm.updateAB()   # compute the matrices from the initial parameters (this would normally happen during training).\n",
    "\n",
    "# While training on ice cream, we will just evaluate the cross-entropy\n",
    "# on the training data itself (icsup), since we are interested in watching it improve.\n",
    "logging.info(\"*** Supervised training on icsup\")\n",
    "loss_sup = lambda model: model_cross_entropy(model, icsup)\n",
    "hmm.train(corpus=icsup, loss=loss_sup, \n",
    "          minibatch_size=10, evalbatch_size=500, lr=0.01, tolerance=0.0001, save_path='icsup_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'icdev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22352/2198366201.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel_error_rate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhmm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_corpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0micdev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mknown_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mictrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'icdev' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "model_error_rate(hmm, eval_corpus=icdev, known_vocab=ictrain.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:*** Unsupervised training on icsup\n",
      "1it [00:00, 79.75it/s]\n",
      "INFO:root:Cross-entropy: 1.1792 nats (= perplexity 3.252)\n",
      "1it [00:00, 115.50it/s]\n",
      "INFO:root:Tagging accuracy: all: 93.939%, known: 93.939%, seen: nan%, novel: nan%\n",
      "1it [00:00, 164.12it/s]\n",
      "INFO:root:Cross-entropy: 1.0874 nats (= perplexity 2.967)\n",
      "1it [00:00, 222.32it/s]\n",
      "INFO:root:Tagging accuracy: all: 90.909%, known: 90.909%, seen: nan%, novel: nan%\n",
      "INFO:root:Saved model to icentire_train.pkl\n",
      "500it [00:05, 92.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# unsupervised \n",
    "logging.info(\"*** Unsupervised training on icsup\")\n",
    "loss_dev = lambda model: model_error_rate(model, eval_corpus=icdev, known_vocab=ictrain.vocab)\n",
    "hmm.train(corpus=ictrain, loss=loss_dev, \n",
    "          minibatch_size=10, evalbatch_size=500, lr=0.01, tolerance=0.0001, save_path='icentire_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A:\n",
      "\tC\tH\t_EOS_TAG_\t_BOS_TAG_\n",
      "C\t0.895\t0.104\t0.001\t0.000\n",
      "H\t0.100\t0.899\t0.001\t0.000\n",
      "_EOS_TAG_\t0.333\t0.333\t0.334\t0.000\n",
      "_BOS_TAG_\t0.409\t0.586\t0.005\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\t_OOV_\n",
      "C\t0.699\t0.194\t0.106\t0.001\n",
      "H\t0.078\t0.300\t0.620\t0.001\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hmm.printAB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invastigate the en data\n",
    "\n",
    "lexicon = build_lexicon(entrain, embeddings_file=Path('../lexicons/words-50.txt'))  # works better with more attributes!\n",
    "hmm1 = HiddenMarkovModel(entrain.tagset, entrain.vocab, lexicon)\n",
    "\n",
    "\n",
    "hmm1 = hmm1.load('en_hmm.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit (system)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
