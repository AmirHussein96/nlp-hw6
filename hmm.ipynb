{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file illustrates how you might experiment with the HMM interface at the prompt.\n",
    "# You can also run it directly.\n",
    "\n",
    "import logging, math, os\n",
    "from pathlib import Path\n",
    "from corpus import TaggedCorpus, desupervise, sentence_str\n",
    "from typing import Callable\n",
    "from corpus import TaggedCorpus, sentence_str\n",
    "\n",
    "from eval import model_cross_entropy, tagger_write_output\n",
    "from hmm import HiddenMarkovModel\n",
    "from lexicon import build_lexicon\n",
    "import torch\n",
    "import pdb\n",
    "from eval import eval_tagging, model_cross_entropy, model_error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 40 tokens from icsup\n",
      "INFO : Created 4 tag types\n",
      "INFO : Created 5 word types\n",
      "INFO : Ice cream vocabulary: ['1', '2', '3', '_EOS_WORD_', '_BOS_WORD_']\n",
      "INFO : Ice cream tagset: ['C', 'H', '_EOS_TAG_', '_BOS_TAG_']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'lexicon'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-60104ddf64bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Ice cream tagset: {list(icsup.tagset)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0micsup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# one-hot lexicon: separate parameters for each word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mhmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHiddenMarkovModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0micsup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0micsup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mhmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_hmm.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*** Current A, B matrices (computed by softmax from small random parameters)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'lexicon'"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(format=\"%(levelname)s : %(message)s\", level=logging.INFO)  # could change INFO to DEBUG\n",
    "# torch.autograd.set_detect_anomaly(True)    # uncomment to improve error messages from .backward(), but slows down\n",
    "\n",
    "# Make an HMM with randomly initialized parameters.\n",
    "icsup = TaggedCorpus(Path(\"../nlp6-data/icsup\"), add_oov=False)\n",
    "logging.info(f\"Ice cream vocabulary: {list(icsup.vocab)}\")\n",
    "logging.info(f\"Ice cream tagset: {list(icsup.tagset)}\")\n",
    "lexicon = build_lexicon(icsup, one_hot=True)   # one-hot lexicon: separate parameters for each word\n",
    "hmm = HiddenMarkovModel(icsup.tagset, icsup.vocab, lexicon)\n",
    "hmm = hmm.load('my_hmm.pkl')\n",
    "logging.info(\"*** Current A, B matrices (computed by softmax from small random parameters)\")\n",
    "#hmm.updateAB()   # compute the matrices from the initial parameters (this would normally happen during training).\n",
    "                 # An alternative is to set them directly to some spreadsheet values you'd like to try.\n",
    "# hmm.A = torch.Tensor([[0.8, 0.1,0.1,0],[0.1,0.8,0.1,0],[0,0,0,0],[0.5, 0.5,0,0]])\n",
    "# hmm.B = torch.Tensor([[0.7, 0.2,0.1],[0.1,0.2,0.7],[0,0,0],[0,0,0]])\n",
    "hmm.printAB()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While training on ice cream, we will just evaluate the cross-entropy\n",
    "# on the training data itself (icsup), since we are interested in watching it improve.\n",
    "logging.info(\"*** Supervised training on icsup\")\n",
    "cross_entropy_loss = lambda model: model_cross_entropy(model, icsup)\n",
    "hmm.train(corpus=icsup, loss=cross_entropy_loss, \n",
    "          minibatch_size=10, evalbatch_size=500, lr=0.01, tolerance=0.0001)\n",
    "\n",
    "logging.info(\"*** A, B matrices after training on icsup (should approximately match initial params on spreadsheet [transposed])\")\n",
    "hmm.printAB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Loading model from my_hmm.pkl\n",
      "INFO : Loaded model from my_hmm.pkl\n",
      "INFO : *** Viterbi results on icraw\n",
      "1it [00:00, 31.56it/s]\n",
      "INFO : *** Forward algorithm on icraw (should approximately match iteration 0 on spreadsheet)\n",
      "INFO : 1.4301312227198852e-58 = p(2 3 3 2 3 2 3 2 2 3 1 3 3 1 1 1 2 1 1 1 3 1 2 1 1 1 2 3 3 2 3 2 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: tensor([-inf, -inf, -inf, 0.]), 1: tensor([3, 3, 3, 3]), 2: tensor([1, 1, 1, 1]), 3: tensor([1, 1, 1, 1]), 4: tensor([1, 1, 1, 1]), 5: tensor([1, 1, 1, 1]), 6: tensor([1, 1, 1, 1]), 7: tensor([1, 1, 1, 1]), 8: tensor([1, 1, 1, 1]), 9: tensor([1, 1, 1, 1]), 10: tensor([1, 1, 1, 1]), 11: tensor([0, 1, 0, 0]), 12: tensor([0, 1, 0, 0]), 13: tensor([0, 1, 1, 1]), 14: tensor([0, 0, 0, 0]), 15: tensor([0, 0, 0, 0]), 16: tensor([0, 0, 0, 0]), 17: tensor([0, 1, 0, 0]), 18: tensor([0, 0, 0, 0]), 19: tensor([0, 0, 0, 0]), 20: tensor([0, 0, 0, 0]), 21: tensor([0, 1, 0, 0]), 22: tensor([0, 0, 0, 0]), 23: tensor([0, 1, 0, 0]), 24: tensor([0, 0, 0, 0]), 25: tensor([0, 0, 0, 0]), 26: tensor([0, 0, 0, 0]), 27: tensor([0, 1, 0, 0]), 28: tensor([0, 1, 0, 0]), 29: tensor([0, 1, 1, 1]), 30: tensor([0, 1, 1, 1]), 31: tensor([1, 1, 1, 1]), 32: tensor([1, 1, 1, 1]), 33: tensor([1, 1, 1, 1]), 34: tensor([0, 1, 1, 1])}\n",
      "_EOS_TAG_\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "3\n",
      "_BOS_TAG_\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "hmm = hmm.load('my_hmm.pkl')\n",
    "logging.info(\"*** Viterbi results on icraw\")\n",
    "icraw = TaggedCorpus(Path(\"../nlp6-data/icraw\"), tagset=icsup.tagset, vocab=icsup.vocab)\n",
    "tagger_write_output(hmm, icraw, Path(\"icraw.output\"))  # calls hmm.viterbi_tagging on each sentence\n",
    "os.system(\"cat ../nlp6-data/icraw.output\")   # print the file we just created, and remove it\n",
    "\n",
    "# Now let's use the forward algorithm to see what the model thinks about \n",
    "# the probability of the spreadsheet \"sentence.\"\n",
    "logging.info(\"*** Forward algorithm on icraw (should approximately match iteration 0 \"\n",
    "             \"on spreadsheet)\")\n",
    "for sentence in icraw:\n",
    "    prob = math.exp(hmm.log_prob(sentence, icraw))\n",
    "    logging.info(f\"{prob} = p({sentence_str(sentence)})\")\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : *** Reestimating on icraw (perplexity should improve on every iteration)\n",
      "1it [00:00, 77.69it/s]\n",
      "INFO : Cross-entropy: 3.9497 nats (= perplexity 51.921)\n",
      "1it [00:00, 139.47it/s]\n",
      "INFO : Cross-entropy: 3.9494 nats (= perplexity 51.905)\n",
      "INFO : Saved model to my_hmm.pkl\n",
      "500it [00:14, 35.62it/s]\n",
      "INFO : *** A, B matrices after reestimation on icraw (SGD, not EM, but still should approximately match final params on spreadsheet [transposed])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A:\n",
      "\tC\tH\t_EOS_TAG_\t_BOS_TAG_\n",
      "C\t0.930\t0.069\t0.001\t0.000\n",
      "H\t0.084\t0.915\t0.001\t0.000\n",
      "_EOS_TAG_\t0.333\t0.334\t0.333\t0.000\n",
      "_BOS_TAG_\t0.072\t0.924\t0.003\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\n",
      "C\t0.650\t0.153\t0.197\n",
      "H\t0.014\t0.484\t0.502\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finally, let's reestimate on the icraw data, as the spreadsheet does.\n",
    "logging.info(\"*** Reestimating on icraw (perplexity should improve on every iteration)\")\n",
    "negative_log_likelihood = lambda model: model_cross_entropy(model, icraw)  # evaluate on icraw itself\n",
    "hmm.train(corpus=icraw, loss=negative_log_likelihood,\n",
    "          minibatch_size=10, evalbatch_size=500, lr=0.001, tolerance=0.0001)\n",
    "\n",
    "logging.info(\"*** A, B matrices after reestimation on icraw (SGD, not EM, but still \"\n",
    "             \"should approximately match final params on spreadsheet [transposed])\")\n",
    "hmm.printAB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hmm on En data\n",
    "# Get the corpora\n",
    "entrain = TaggedCorpus(Path(\"../nlp6-data/ensup\"), Path(\"../nlp6-data/enraw\"))                               # all training\n",
    "ensup =   TaggedCorpus(Path(\"../nlp6-data/ensup\"), tagset=entrain.tagset, vocab=entrain.vocab)  # supervised training\n",
    "endev =   TaggedCorpus(Path(\"../nlp6-data/endev\"), tagset=entrain.tagset, vocab=entrain.vocab)  # evaluation\n",
    "logging.info(f\"Tagset: f{list(entrain.tagset)}\")\n",
    "\n",
    "known_vocab = TaggedCorpus(Path(\"../nlp6-data/ensup\")).vocab    # words seen with supervised tags; used in evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an HMM\n",
    "lexicon = build_lexicon(entrain, embeddings_file=Path('../lexicons/words-50.txt'))  # works better with more attributes!\n",
    "hmm = HiddenMarkovModel(entrain.tagset, entrain.vocab, lexicon)\n",
    "\n",
    "\n",
    "hmm = hmm.load('my_hmm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "996it [00:04, 199.85it/s]\n",
      "996it [00:03, 278.55it/s]\n",
      "996it [00:05, 195.63it/s]\n",
      "996it [00:03, 279.85it/s]\n",
      "10000it [05:17, 31.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# train on unsupervised\n",
    "loss_dev = lambda model: model_error_rate(model, eval_corpus=endev, known_vocab=known_vocab)\n",
    "hmm.train(corpus=entrain, loss=loss_dev, minibatch_size=30, evalbatch_size=10000, lr=0.0001, reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Gold:    ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/N added/V ,/, ``/` and/C that/D means/V virtually/R everyone/N who/W works/V here/R ./.\n",
      "INFO:root:Viterbi: ``/P We/V 're/R strongly/- _OOV_/W that/U anyone/W who/V has/V eaten/I in/D the/N cafeteria/D this/N month/V have/D the/N shot/, ,/' ''/' Mr./C Mattausch/N added/, ,/C ``/C and/W that/V means/R virtually/U everyone/W who/S works/R here/. ./'\n",
      "INFO:root:Loss:    31/36\n",
      "INFO:root:Prob:    1.1398958712484866e-125\n",
      "INFO:root:Gold:    I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/N ``/` _OOV_/N 's/P _OOV_/N _OOV_/N ./. ''/'\n",
      "INFO:root:Viterbi: I/D was/N _OOV_/T to/V read/D the/N _OOV_/I of/N facts/I in/J your/N Oct./N 13/N editorial/N ``/N _OOV_/P 's/N _OOV_/N _OOV_/. ./' ''/.\n",
      "INFO:root:Loss:    18/23\n",
      "INFO:root:Prob:    1.0263348879662016e-81\n",
      "INFO:root:Gold:    It/P is/V the/D _OOV_/J guerrillas/N who/W are/V aligned/V with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO:root:Viterbi: It/I is/D the/N _OOV_/N guerrillas/W who/V are/N aligned/I with/D the/N drug/N traffickers/, ,/V not/D the/J left/N _OOV_/. ./'\n",
      "INFO:root:Loss:    17/20\n",
      "INFO:root:Prob:    6.92796021424544e-61\n",
      "INFO:root:Gold:    This/D information/N was/V _OOV_/V from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO:root:Viterbi: This/N information/I was/N _OOV_/I from/U your/P own/N news/N stories/I on/D the/N region/. ./'\n",
      "INFO:root:Loss:    11/15\n",
      "INFO:root:Prob:    1.410965011733926e-50\n",
      "INFO:root:Gold:    _OOV_/J _OOV_/J government/N _OOV_/N of/I the/D ``/` _OOV_/F ''/' was/V due/J to/T the/D drug/N _OOV_/N '/P history/N of/I _OOV_/V out/R _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO:root:Viterbi: _OOV_/N _OOV_/N government/N _OOV_/I of/D the/N ``/C _OOV_/' ''/V was/V due/T to/D the/N drug/N _OOV_/P '/N history/I of/N _OOV_/I out/N _OOV_/I in/D the/N _OOV_/. ./'\n",
      "INFO:root:Loss:    22/27\n",
      "INFO:root:Prob:    5.265675001008311e-97\n",
      "INFO:root:Gold:    Mary/N _OOV_/N Palo/N Alto/N ,/, Calif/N ./.\n",
      "INFO:root:Viterbi: Mary/N _OOV_/N Palo/N Alto/, ,/N Calif/. ./'\n",
      "INFO:root:Loss:    4/9\n",
      "INFO:root:Prob:    5.825171615973573e-29\n",
      "INFO:root:Gold:    I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/R well/R as/I other/J U.S./N news/N publications/N of/I like/J mind/N -RRB-/- should/M put/V its/P money/N where/W its/P mouth/N is/V :/: _OOV_/V computer/N equipment/N to/T replace/V that/I damaged/V at/I El/N _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/N _OOV_/N journalists/N ./.\n",
      "INFO:root:Viterbi: I/E suggest/W that/D The/N Wall/N Street/N Journal/N -LRB-/V as/R well/V as/J other/N U.S./N news/N publications/I of/J like/N mind/N -RRB-/M should/V put/D its/N money/I where/D its/N mouth/I is/: :/C _OOV_/N computer/N equipment/T to/V replace/W that/V damaged/I at/N El/N _OOV_/, ,/V buy/N ad/N space/, ,/N publish/N stories/I under/D the/N _OOV_/I of/N El/N _OOV_/N journalists/. ./'\n",
      "INFO:root:Loss:    41/55\n",
      "INFO:root:Prob:    9.43565747933535e-209\n",
      "INFO:root:Gold:    Perhaps/R an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/V ''/' El/N _OOV_/N journalists/N and/C staff/N by/I paying/V for/I added/V security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO:root:Viterbi: Perhaps/D an/N arrangement/M could/V be/V worked/R out/T to/C ``/N sponsor/' ''/C El/C _OOV_/N journalists/I and/N staff/I by/V paying/I for/J added/N security/I in/N exchange/I for/J exclusive/N stories/. ./'\n",
      "INFO:root:Loss:    25/29\n",
      "INFO:root:Prob:    4.670467796644033e-106\n",
      "INFO:root:Gold:    _OOV_/V El/N _OOV_/N 's/P courage/N with/I real/J support/N ./.\n",
      "INFO:root:Viterbi: _OOV_/` El/` _OOV_/P 's/N courage/I with/J real/N support/. ./'\n",
      "INFO:root:Loss:    9/11\n",
      "INFO:root:Prob:    2.8370637189145197e-39\n",
      "INFO:root:Gold:    Douglas/N B./N Evans/N\n",
      "INFO:root:Viterbi: Douglas/N B./N Evans/.\n",
      "INFO:root:Loss:    1/5\n",
      "INFO:root:Prob:    1.9848938504775886e-16\n"
     ]
    }
   ],
   "source": [
    "logger=logging.getLogger() \n",
    "\n",
    "#Now we are going to Set the threshold of logger to DEBUG \n",
    "logger.setLevel(logging.DEBUG) \n",
    "\n",
    "for m, sentence in enumerate(endev):\n",
    "    if m >= 10: break\n",
    "    viterbi = hmm.viterbi_tagging(desupervise(sentence), endev)\n",
    "    counts = eval_tagging(predicted=viterbi, gold=sentence, \n",
    "                          known_vocab=known_vocab)\n",
    "    num = counts['NUM', 'ALL']\n",
    "    denom = counts['DENOM', 'ALL']\n",
    "    \n",
    "    logging.info(f\"Gold:    {sentence_str(sentence)}\")\n",
    "    logging.info(f\"Viterbi: {sentence_str(viterbi)}\")\n",
    "    logging.info(f\"Loss:    {denom - num}/{denom}\")\n",
    "    logging.info(f\"Prob:    {math.exp(hmm.log_prob(sentence, endev))}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ddf02f4c0292df6831d782027890daba6803fd8ec186dc3e6f7222b70d5e314"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
