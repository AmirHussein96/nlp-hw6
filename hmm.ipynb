{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file illustrates how you might experiment with the HMM interface at the prompt.\n",
    "# You can also run it directly.\n",
    "\n",
    "import logging, math, os\n",
    "from pathlib import Path\n",
    "from corpus import TaggedCorpus, desupervise, sentence_str\n",
    "from typing import Callable\n",
    "from corpus import TaggedCorpus, sentence_str\n",
    "\n",
    "from eval import model_cross_entropy, tagger_write_output\n",
    "from hmm import HiddenMarkovModel\n",
    "from lexicon import build_lexicon\n",
    "import torch\n",
    "\n",
    "import pdb\n",
    "from eval import eval_tagging, model_cross_entropy, model_error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 40 tokens from icsup\n",
      "INFO : Created 4 tag types\n",
      "INFO : Created 5 word types\n",
      "INFO : Ice cream vocabulary: ['1', '2', '3', '_EOS_WORD_', '_BOS_WORD_']\n",
      "INFO : Ice cream tagset: ['C', 'H', '_EOS_TAG_', '_BOS_TAG_']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'lexicon'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-60104ddf64bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Ice cream tagset: {list(icsup.tagset)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0micsup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# one-hot lexicon: separate parameters for each word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mhmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHiddenMarkovModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0micsup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0micsup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mhmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_hmm.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*** Current A, B matrices (computed by softmax from small random parameters)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'lexicon'"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(format=\"%(levelname)s : %(message)s\", level=logging.INFO)  # could change INFO to DEBUG\n",
    "# torch.autograd.set_detect_anomaly(True)    # uncomment to improve error messages from .backward(), but slows down\n",
    "\n",
    "# Make an HMM with randomly initialized parameters.\n",
    "icsup = TaggedCorpus(Path(\"../nlp6-data/icsup\"), add_oov=False)\n",
    "logging.info(f\"Ice cream vocabulary: {list(icsup.vocab)}\")\n",
    "logging.info(f\"Ice cream tagset: {list(icsup.tagset)}\")\n",
    "lexicon = build_lexicon(icsup, one_hot=True)   # one-hot lexicon: separate parameters for each word\n",
    "hmm = HiddenMarkovModel(icsup.tagset, icsup.vocab, lexicon)\n",
    "hmm = hmm.load('my_hmm.pkl')\n",
    "logging.info(\"*** Current A, B matrices (computed by softmax from small random parameters)\")\n",
    "#hmm.updateAB()   # compute the matrices from the initial parameters (this would normally happen during training).\n",
    "                 # An alternative is to set them directly to some spreadsheet values you'd like to try.\n",
    "# hmm.A = torch.Tensor([[0.8, 0.1,0.1,0],[0.1,0.8,0.1,0],[0,0,0,0],[0.5, 0.5,0,0]])\n",
    "# hmm.B = torch.Tensor([[0.7, 0.2,0.1],[0.1,0.2,0.7],[0,0,0],[0,0,0]])\n",
    "hmm.printAB()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While training on ice cream, we will just evaluate the cross-entropy\n",
    "# on the training data itself (icsup), since we are interested in watching it improve.\n",
    "logging.info(\"*** Supervised training on icsup\")\n",
    "cross_entropy_loss = lambda model: model_cross_entropy(model, icsup)\n",
    "hmm.train(corpus=icsup, loss=cross_entropy_loss, \n",
    "          minibatch_size=10, evalbatch_size=500, lr=0.01, tolerance=0.0001)\n",
    "\n",
    "logging.info(\"*** A, B matrices after training on icsup (should approximately match initial params on spreadsheet [transposed])\")\n",
    "hmm.printAB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Loading model from my_hmm.pkl\n",
      "INFO : Loaded model from my_hmm.pkl\n",
      "INFO : *** Viterbi results on icraw\n",
      "1it [00:00, 31.56it/s]\n",
      "INFO : *** Forward algorithm on icraw (should approximately match iteration 0 on spreadsheet)\n",
      "INFO : 1.4301312227198852e-58 = p(2 3 3 2 3 2 3 2 2 3 1 3 3 1 1 1 2 1 1 1 3 1 2 1 1 1 2 3 3 2 3 2 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: tensor([-inf, -inf, -inf, 0.]), 1: tensor([3, 3, 3, 3]), 2: tensor([1, 1, 1, 1]), 3: tensor([1, 1, 1, 1]), 4: tensor([1, 1, 1, 1]), 5: tensor([1, 1, 1, 1]), 6: tensor([1, 1, 1, 1]), 7: tensor([1, 1, 1, 1]), 8: tensor([1, 1, 1, 1]), 9: tensor([1, 1, 1, 1]), 10: tensor([1, 1, 1, 1]), 11: tensor([0, 1, 0, 0]), 12: tensor([0, 1, 0, 0]), 13: tensor([0, 1, 1, 1]), 14: tensor([0, 0, 0, 0]), 15: tensor([0, 0, 0, 0]), 16: tensor([0, 0, 0, 0]), 17: tensor([0, 1, 0, 0]), 18: tensor([0, 0, 0, 0]), 19: tensor([0, 0, 0, 0]), 20: tensor([0, 0, 0, 0]), 21: tensor([0, 1, 0, 0]), 22: tensor([0, 0, 0, 0]), 23: tensor([0, 1, 0, 0]), 24: tensor([0, 0, 0, 0]), 25: tensor([0, 0, 0, 0]), 26: tensor([0, 0, 0, 0]), 27: tensor([0, 1, 0, 0]), 28: tensor([0, 1, 0, 0]), 29: tensor([0, 1, 1, 1]), 30: tensor([0, 1, 1, 1]), 31: tensor([1, 1, 1, 1]), 32: tensor([1, 1, 1, 1]), 33: tensor([1, 1, 1, 1]), 34: tensor([0, 1, 1, 1])}\n",
      "_EOS_TAG_\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "0\n",
      "C\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "1\n",
      "H\n",
      "3\n",
      "_BOS_TAG_\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "hmm = hmm.load('my_hmm.pkl')\n",
    "logging.info(\"*** Viterbi results on icraw\")\n",
    "icraw = TaggedCorpus(Path(\"../nlp6-data/icraw\"), tagset=icsup.tagset, vocab=icsup.vocab)\n",
    "tagger_write_output(hmm, icraw, Path(\"icraw.output\"))  # calls hmm.viterbi_tagging on each sentence\n",
    "os.system(\"cat ../nlp6-data/icraw.output\")   # print the file we just created, and remove it\n",
    "\n",
    "# Now let's use the forward algorithm to see what the model thinks about \n",
    "# the probability of the spreadsheet \"sentence.\"\n",
    "logging.info(\"*** Forward algorithm on icraw (should approximately match iteration 0 \"\n",
    "             \"on spreadsheet)\")\n",
    "for sentence in icraw:\n",
    "    prob = math.exp(hmm.log_prob(sentence, icraw))\n",
    "    logging.info(f\"{prob} = p({sentence_str(sentence)})\")\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : *** Reestimating on icraw (perplexity should improve on every iteration)\n",
      "1it [00:00, 77.69it/s]\n",
      "INFO : Cross-entropy: 3.9497 nats (= perplexity 51.921)\n",
      "1it [00:00, 139.47it/s]\n",
      "INFO : Cross-entropy: 3.9494 nats (= perplexity 51.905)\n",
      "INFO : Saved model to my_hmm.pkl\n",
      "500it [00:14, 35.62it/s]\n",
      "INFO : *** A, B matrices after reestimation on icraw (SGD, not EM, but still should approximately match final params on spreadsheet [transposed])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A:\n",
      "\tC\tH\t_EOS_TAG_\t_BOS_TAG_\n",
      "C\t0.930\t0.069\t0.001\t0.000\n",
      "H\t0.084\t0.915\t0.001\t0.000\n",
      "_EOS_TAG_\t0.333\t0.334\t0.333\t0.000\n",
      "_BOS_TAG_\t0.072\t0.924\t0.003\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\n",
      "C\t0.650\t0.153\t0.197\n",
      "H\t0.014\t0.484\t0.502\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finally, let's reestimate on the icraw data, as the spreadsheet does.\n",
    "logging.info(\"*** Reestimating on icraw (perplexity should improve on every iteration)\")\n",
    "negative_log_likelihood = lambda model: model_cross_entropy(model, icraw)  # evaluate on icraw itself\n",
    "hmm.train(corpus=icraw, loss=negative_log_likelihood,\n",
    "          minibatch_size=10, evalbatch_size=500, lr=0.001, tolerance=0.0001)\n",
    "\n",
    "logging.info(\"*** A, B matrices after reestimation on icraw (SGD, not EM, but still \"\n",
    "             \"should approximately match final params on spreadsheet [transposed])\")\n",
    "hmm.printAB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hmm on En data\n",
    "# Get the corpora\n",
    "entrain = TaggedCorpus(Path(\"../nlp6-data/ensup\"), Path(\"../nlp6-data/enraw\"))                               # all training\n",
    "ensup =   TaggedCorpus(Path(\"../nlp6-data/ensup\"), tagset=entrain.tagset, vocab=entrain.vocab)  # supervised training\n",
    "endev =   TaggedCorpus(Path(\"../nlp6-data/endev\"), tagset=entrain.tagset, vocab=entrain.vocab)  # evaluation\n",
    "logging.info(f\"Tagset: f{list(entrain.tagset)}\")\n",
    "\n",
    "known_vocab = TaggedCorpus(Path(\"../nlp6-data/ensup\")).vocab    # words seen with supervised tags; used in evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an HMM\n",
    "lexicon = build_lexicon(entrain, embeddings_file=Path('../lexicons/words-50.txt'))  # works better with more attributes!\n",
    "hmm = HiddenMarkovModel(entrain.tagset, entrain.vocab, lexicon)\n",
    "\n",
    "\n",
    "hmm = hmm.load('en_hmm.pkl')\n",
    "#hmm = hmm.load('divsup_train.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "996it [00:04, 238.68it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/local/QCRI/ahussein/nlp_hw/nlp-hw6/hmm.py\u001b[0m(234)\u001b[0;36mviterbi_tagging\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    232 \u001b[0;31m            \u001b[0mwi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    233 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 234 \u001b[0;31m            \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    235 \u001b[0;31m            \u001b[0mmax_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    236 \u001b[0;31m            \u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# alpha values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1;32m    229 \u001b[0m        \u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m  \u001b[0;31m# handling the first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    230 \u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    231 \u001b[0m            \u001b[0;31m# alpha[j+1] =  torch.matmul(alpha[j],self.A) * self.B[:,sent[j+1][0]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    232 \u001b[0m            \u001b[0mwi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    233 \u001b[0m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 234 \u001b[0;31m            \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    235 \u001b[0m            \u001b[0mmax_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    236 \u001b[0m            \u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# alpha values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    237 \u001b[0m            \u001b[0mbackpointer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    238 \u001b[0m        \u001b[0;31m# handeling the last tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    239 \u001b[0m        \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "> \u001b[0;32m/home/local/QCRI/ahussein/nlp_hw/nlp-hw6/hmm.py\u001b[0m(233)\u001b[0;36mviterbi_tagging\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    231 \u001b[0;31m            \u001b[0;31m# alpha[j+1] =  torch.matmul(alpha[j],self.A) * self.B[:,sent[j+1][0]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    232 \u001b[0;31m            \u001b[0mwi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 233 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    234 \u001b[0;31m            \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    235 \u001b[0;31m            \u001b[0mmax_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:08, ?it/s]\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f75afada6909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_error_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_corpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknown_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mknown_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/nlp_hw/nlp-hw6/eval.py\u001b[0m in \u001b[0;36mmodel_error_rate\u001b[0;34m(model, eval_corpus, known_vocab)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mcounts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# keep running totals here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviterbi_tagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesupervise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mcounts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0meval_tagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknown_vocab\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# += works on dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlp_hw/nlp-hw6/hmm.py\u001b[0m in \u001b[0;36mviterbi_tagging\u001b[0;34m(self, sentence, corpus)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;31m# alpha[j+1] =  torch.matmul(alpha[j],self.A) * self.B[:,sent[j+1][0]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mwi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mmax_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlp_hw/nlp-hw6/hmm.py\u001b[0m in \u001b[0;36mviterbi_tagging\u001b[0;34m(self, sentence, corpus)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;31m# alpha[j+1] =  torch.matmul(alpha[j],self.A) * self.B[:,sent[j+1][0]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mwi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mmax_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_error_rate(hmm, eval_corpus=endev, known_vocab=known_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "996it [00:04, 199.85it/s]\n",
      "996it [00:03, 278.55it/s]\n",
      "996it [00:05, 195.63it/s]\n",
      "996it [00:03, 279.85it/s]\n",
      "10000it [05:17, 31.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# train on unsupervised\n",
    "loss_dev = lambda model: model_error_rate(model, eval_corpus=endev, known_vocab=known_vocab)\n",
    "hmm.train(corpus=entrain, loss=loss_dev, minibatch_size=30, evalbatch_size=10000, lr=0.0001, reg=0,save_path='unsup_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Gold:    ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/N added/V ,/, ``/` and/C that/D means/V virtually/R everyone/N who/W works/V here/R ./.\n",
      "INFO:root:Viterbi: ``/P We/V 're/R strongly/- _OOV_/W that/U anyone/W who/V has/V eaten/I in/D the/N cafeteria/D this/N month/V have/D the/N shot/, ,/' ''/' Mr./C Mattausch/N added/, ,/C ``/C and/W that/V means/R virtually/U everyone/W who/S works/R here/. ./'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1388888888888889\n",
      "0.8611111111111112\n"
     ]
    }
   ],
   "source": [
    "logger=logging.getLogger() \n",
    "\n",
    "#Now we are going to Set the threshold of logger to DEBUG \n",
    "logger.setLevel(logging.DEBUG) \n",
    "acc = 0\n",
    "c =0\n",
    "for m, sentence in enumerate(endev):\n",
    "    if m >= 1: break\n",
    "    viterbi = hmm.viterbi_tagging(desupervise(sentence), endev)\n",
    "    counts = eval_tagging(predicted=viterbi, gold=sentence, \n",
    "                          known_vocab=known_vocab)\n",
    "    num = counts['NUM', 'ALL']\n",
    "    denom = counts['DENOM', 'ALL']\n",
    "  \n",
    "    logging.info(f\"Gold:    {sentence_str(sentence)}\")\n",
    "    logging.info(f\"Viterbi: {sentence_str(viterbi)}\")\n",
    "    acc_all = (denom - num)/denom\n",
    "    print(num/denom)\n",
    "   # logging.info(f\"acc:    {num}/{denom}\")\n",
    "    acc += acc_all \n",
    "    \n",
    "    c+=1\n",
    "print(acc/c)\n",
    "    # logging.info(f\"Prob:    {math.exp(hmm.log_prob(sentence, endev))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('DENOM', 'KNOWN'): 33,\n",
       "         ('NUM', 'KNOWN'): 4,\n",
       "         ('DENOM', 'ALL'): 36,\n",
       "         ('NUM', 'ALL'): 5,\n",
       "         ('DENOM', 'NOVEL'): 1,\n",
       "         ('DENOM', 'SEEN'): 2,\n",
       "         ('NUM', 'SEEN'): 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hmm on ic data\n",
    "# Get the corpora\n",
    "ictrain = TaggedCorpus(Path(\"../nlp6-data/icsup\"), Path(\"../nlp6-data/icraw\"))                               # all training\n",
    "icsup =   TaggedCorpus(Path(\"../nlp6-data/icsup\"), tagset=ictrain.tagset, vocab=ictrain.vocab)  # supervised training\n",
    "icdev =   TaggedCorpus(Path(\"../nlp6-data/icdev\"), tagset=ictrain.tagset, vocab=ictrain.vocab)  # evaluation\n",
    "logging.info(f\"Tagset: f{list(ictrain.tagset)}\")\n",
    "\n",
    "known_vocab = TaggedCorpus(Path(\"../nlp6-data/icsup\")).vocab    # words seen with supervised tags; used in evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:*** Current A, B matrices (computed by softmax from small random parameters)\n",
      "INFO:root:*** Supervised training on icsup\n",
      "4it [00:00, 232.36it/s]\n",
      "INFO:root:Cross-entropy: 2.2305 nats (= perplexity 9.304)\n",
      "4it [00:00, 518.70it/s]\n",
      "INFO:root:Cross-entropy: 1.1334 nats (= perplexity 3.106)\n",
      "4it [00:00, 525.57it/s]\n",
      "INFO:root:Cross-entropy: 1.1079 nats (= perplexity 3.028)\n",
      "4it [00:00, 530.35it/s]\n",
      "INFO:root:Cross-entropy: 1.0982 nats (= perplexity 2.999)\n",
      "4it [00:00, 511.55it/s]\n",
      "INFO:root:Cross-entropy: 1.0931 nats (= perplexity 2.984)\n",
      "4it [00:00, 502.97it/s]\n",
      "INFO:root:Cross-entropy: 1.0900 nats (= perplexity 2.974)\n",
      "4it [00:00, 500.38it/s]\n",
      "INFO:root:Cross-entropy: 1.0879 nats (= perplexity 2.968)\n",
      "4it [00:00, 520.21it/s]\n",
      "INFO:root:Cross-entropy: 1.0864 nats (= perplexity 2.964)\n",
      "4it [00:00, 509.00it/s]\n",
      "INFO:root:Cross-entropy: 1.0853 nats (= perplexity 2.960)\n",
      "4it [00:00, 508.23it/s]\n",
      "INFO:root:Cross-entropy: 1.0844 nats (= perplexity 2.958)\n",
      "4it [00:00, 520.72it/s]\n",
      "INFO:root:Cross-entropy: 1.0837 nats (= perplexity 2.956)\n",
      "4it [00:00, 526.05it/s]\n",
      "INFO:root:Cross-entropy: 1.0832 nats (= perplexity 2.954)\n",
      "4it [00:00, 528.77it/s]\n",
      "INFO:root:Cross-entropy: 1.0827 nats (= perplexity 2.953)\n",
      "4it [00:00, 535.62it/s]\n",
      "INFO:root:Cross-entropy: 1.0823 nats (= perplexity 2.951)\n",
      "4it [00:00, 530.94it/s]\n",
      "INFO:root:Cross-entropy: 1.0819 nats (= perplexity 2.950)\n",
      "4it [00:00, 535.24it/s]\n",
      "INFO:root:Cross-entropy: 1.0816 nats (= perplexity 2.949)\n",
      "4it [00:00, 482.88it/s]\n",
      "INFO:root:Cross-entropy: 1.0813 nats (= perplexity 2.949)\n",
      "4it [00:00, 514.32it/s]\n",
      "INFO:root:Cross-entropy: 1.0811 nats (= perplexity 2.948)\n",
      "4it [00:00, 517.22it/s]\n",
      "INFO:root:Cross-entropy: 1.0809 nats (= perplexity 2.947)\n",
      "4it [00:00, 389.42it/s]\n",
      "INFO:root:Cross-entropy: 1.0807 nats (= perplexity 2.947)\n",
      "4it [00:00, 399.72it/s]\n",
      "INFO:root:Cross-entropy: 1.0805 nats (= perplexity 2.946)\n",
      "4it [00:00, 414.41it/s]\n",
      "INFO:root:Cross-entropy: 1.0804 nats (= perplexity 2.946)\n",
      "4it [00:00, 526.69it/s]\n",
      "INFO:root:Cross-entropy: 1.0803 nats (= perplexity 2.945)\n",
      "4it [00:00, 536.30it/s]\n",
      "INFO:root:Cross-entropy: 1.0801 nats (= perplexity 2.945)\n",
      "4it [00:00, 520.09it/s]\n",
      "INFO:root:Cross-entropy: 1.0800 nats (= perplexity 2.945)\n",
      "4it [00:00, 485.21it/s]\n",
      "INFO:root:Cross-entropy: 1.0799 nats (= perplexity 2.944)\n",
      "INFO:root:Saved model to icsup_train.pkl\n",
      "12500it [01:34, 131.91it/s]\n"
     ]
    }
   ],
   "source": [
    "logger=logging.getLogger() \n",
    "\n",
    "#Now we are going to Set the threshold of logger to DEBUG \n",
    "logger.setLevel(logging.INFO) \n",
    "lexicon = build_lexicon(ictrain, one_hot=True)   # one-hot lexicon: separate parameters for each word\n",
    "hmm = HiddenMarkovModel(ictrain.tagset, ictrain.vocab, lexicon)\n",
    "\n",
    "logging.info(\"*** Current A, B matrices (computed by softmax from small random parameters)\")\n",
    "hmm.updateAB()   # compute the matrices from the initial parameters (this would normally happen during training).\n",
    "\n",
    "# While training on ice cream, we will just evaluate the cross-entropy\n",
    "# on the training data itself (icsup), since we are interested in watching it improve.\n",
    "logging.info(\"*** Supervised training on icsup\")\n",
    "loss_sup = lambda model: model_cross_entropy(model, icsup)\n",
    "hmm.train(corpus=icsup, loss=loss_sup, \n",
    "          minibatch_size=10, evalbatch_size=500, lr=0.01, tolerance=0.0001, save_path='icsup_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'icdev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-63b10c79bbc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_error_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_corpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0micdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknown_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mictrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'icdev' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "model_error_rate(hmm, eval_corpus=icdev, known_vocab=ictrain.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:*** Unsupervised training on icsup\n",
      "1it [00:00, 79.75it/s]\n",
      "INFO:root:Cross-entropy: 1.1792 nats (= perplexity 3.252)\n",
      "1it [00:00, 115.50it/s]\n",
      "INFO:root:Tagging accuracy: all: 93.939%, known: 93.939%, seen: nan%, novel: nan%\n",
      "1it [00:00, 164.12it/s]\n",
      "INFO:root:Cross-entropy: 1.0874 nats (= perplexity 2.967)\n",
      "1it [00:00, 222.32it/s]\n",
      "INFO:root:Tagging accuracy: all: 90.909%, known: 90.909%, seen: nan%, novel: nan%\n",
      "INFO:root:Saved model to icentire_train.pkl\n",
      "500it [00:05, 92.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# unsupervised \n",
    "logging.info(\"*** Unsupervised training on icsup\")\n",
    "loss_dev = lambda model: model_error_rate(model, eval_corpus=icdev, known_vocab=ictrain.vocab)\n",
    "hmm.train(corpus=ictrain, loss=loss_dev, \n",
    "          minibatch_size=10, evalbatch_size=500, lr=0.01, tolerance=0.0001, save_path='icentire_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A:\n",
      "\tC\tH\t_EOS_TAG_\t_BOS_TAG_\n",
      "C\t0.895\t0.104\t0.001\t0.000\n",
      "H\t0.100\t0.899\t0.001\t0.000\n",
      "_EOS_TAG_\t0.333\t0.333\t0.334\t0.000\n",
      "_BOS_TAG_\t0.409\t0.586\t0.005\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\t_OOV_\n",
      "C\t0.699\t0.194\t0.106\t0.001\n",
      "H\t0.078\t0.300\t0.620\t0.001\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hmm.printAB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invastigate the en data\n",
    "\n",
    "lexicon = build_lexicon(entrain, embeddings_file=Path('../lexicons/words-50.txt'))  # works better with more attributes!\n",
    "hmm1 = HiddenMarkovModel(entrain.tagset, entrain.vocab, lexicon)\n",
    "\n",
    "\n",
    "hmm1 = hmm1.load('en_hmm.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ddf02f4c0292df6831d782027890daba6803fd8ec186dc3e6f7222b70d5e314"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
